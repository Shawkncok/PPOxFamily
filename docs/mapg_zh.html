<!DOCTYPE html>
<html><head><meta charset="utf-8"></meta><title>Annonated Algorithm Visualization</title><link rel="stylesheet" href="pylit.css?v=1"></link><link rel="stylesheet" href="solarized.css"></link><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" integrity="sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/" crossorigin="anonymous"></link><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js" integrity="sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);" defer="True"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.css"></link><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.js"></script><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/mode/python/python.min.js"></script></head><body><div class="section" id="section-0"><div class="docs doc-strings"><p><p><a href="index.html"><b>HOME<br></b></a></p></p><a href="https://github.com/opendilab/PPOxFamily" target="_blank"><img alt="GitHub" style="max-width:100%;" src="https://img.shields.io/github/stars/opendilab/PPOxFamily?style=social"></img></a>  <a href="https://space.bilibili.com/1112854351?spm_id_from=333.337.0.0" target="_blank"><img alt="bilibili" style="max-width:100%;" src="https://img.shields.io/badge/bilibili-video%20course-blue"></img></a>  <a href="https://twitter.com/OpenDILab" rel="nofollow" target="_blank"><img alt="twitter" style="max-width:100%;" src="https://img.shields.io/twitter/follow/opendilab?style=social"></img></a><br><a href="https://github.com/opendilab/PPOxFamily/tree/main/chapter6_marl/mapg_zh.py" target="_blank">View code on GitHub</a><br><br>这是一个关于多智能体合作场景中集中式训练和分布式执行 (centralized training and decentralized execution, CTDE) 策略梯度算法的 PyTorch 教程。<br>此教程使用了在 <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">marl_network</span> 中定义的 CTDEActorCriticNetwork 和在 <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">pg</span> 中定义的损失函数。主函数描述了使用伪数据的 MARL CTDE 策略梯度算法的核心部分。<br>有关多智能体合作强化学习的更多细节，可以在<a href="https://github.com/opendilab/PPOxFamily/blob/main/chapter6_marl/chapter6_lecture.pdf">Related Link</a> 中找到。<br><br>这个教程主要由两部分组成，你可以按顺序学习这些部分，或者跳转到你感兴趣的部分：<br>  - 针对多智能体强化学习的 CTDE 策略梯度算法<br>  - 针对多智能体强化学习的 CTDE Actor-Critic 算法</div></div><div class="section" id="section-2"><div class="docs doc-strings"><p>    <b>mapg_training_opeator 功能概述</b><br>        关于 CTDE 策略梯度算法训练过程的主函数。<br>        定义一些超参数，神经网络和优化器，然后生成假数据并计算策略梯度损失。<br>        最后，使用优化器更新网络参数。在实际实践中，这些伪数据应该被与环境交互得到的真实数据替换。</p></div><div class="code"><pre><code id="code_2" name="py_code">from pg import pg_data, pg_error


def mapg_training_opeator() -> None:</code></pre></div></div><div class="section" id="section-4"><div class="docs doc-strings"><p>    设置必要的超参数。</p></div><div class="code"><pre><code id="code_4" name="py_code">    batch_size, agent_num, local_state_shape, global_state_shape, action_shape = 4, 5, 10, 20, 6</code></pre></div></div><div class="section" id="section-5"><div class="docs doc-strings"><p>    Entropy bonus 的权重，有助于智能体进行探索。</p></div><div class="code"><pre><code id="code_5" name="py_code">    entropy_weight = 0.001</code></pre></div></div><div class="section" id="section-6"><div class="docs doc-strings"><p>    对未来奖励的折扣因子</p></div><div class="code"><pre><code id="code_6" name="py_code">    discount_factor = 0.99</code></pre></div></div><div class="section" id="section-7"><div class="docs doc-strings"><p>    根据运行环境设定，决定 tensor 放置于 cpu 或是 cuda 。</p></div><div class="code"><pre><code id="code_7" name="py_code">    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
</code></pre></div></div><div class="section" id="section-8"><div class="docs doc-strings"><p>    定义 CTDE 多智能体神经网络和优化器。</p></div><div class="code"><pre><code id="code_8" name="py_code">    model = CTDEActorCriticNetwork(agent_num, local_state_shape, global_state_shape, action_shape)
    model.to(device)</code></pre></div></div><div class="section" id="section-9"><div class="docs doc-strings"><p>    Adam 是深度强化学习中最常用的优化器。如果你想使用 weight decay，你应当使用 <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">torch.optim.AdamW</span> 优化器</p></div><div class="code"><pre><code id="code_9" name="py_code">    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
</code></pre></div></div><div class="section" id="section-10"><div class="docs doc-strings"><p>    定义对应随机生成的伪数据，其格式与真实和环境交互得到的数据相同。<br>    要注意，数据和网络应当在相同的设备上 (cpu 或 cuda)。<br>    简单起见，我们这里假定一个 batch 的数据构成了一个完整的 episode。<br>    在真实实践中，一个训练 batch 可能是多个 episode 混在一起的结果。我们常常使用 <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">done</span> 这个变量来区分不同的 episodes。</p></div><div class="code"><pre><code id="code_10" name="py_code">    local_state = torch.randn(batch_size, agent_num, local_state_shape).to(device)
    global_state = torch.randn(batch_size, global_state_shape).to(device)
    action = torch.randint(0, action_shape, (batch_size, agent_num)).to(device)
    reward = torch.randn(batch_size, agent_num).to(device)</code></pre></div></div><div class="section" id="section-11"><div class="docs doc-strings"><p>    对于最基础的策略梯度算法，累积回报值由带折扣因子的累积奖励计算得来。</p></div><div class="code"><pre><code id="code_11" name="py_code">    return_ = torch.zeros_like(reward)
    for i in reversed(range(batch_size)):
        return_[i] = reward[i] + (discount_factor * return_[i + 1] if i + 1 < batch_size else 0)
</code></pre></div></div><div class="section" id="section-12"><div class="docs doc-strings"><p>    Actor-Critic 网络前向计算.</p></div><div class="code"><pre><code id="code_12" name="py_code">    output = model(local_state, global_state)</code></pre></div></div><div class="section" id="section-13"><div class="docs doc-strings"><p>    准备用于计算策略梯度损失函数的数据。</p></div><div class="code"><pre><code id="code_13" name="py_code">    data = pg_data(output.logit, action, return_)</code></pre></div></div><div class="section" id="section-14"><div class="docs doc-strings"><p>    计算策略梯度算法的损失函数。</p></div><div class="code"><pre><code id="code_14" name="py_code">    loss = pg_error(data)</code></pre></div></div><div class="section" id="section-15"><div class="docs doc-strings"><p>    策略损失函数部分和熵损失函数部分的加权和。<br>    注意，这里我们只使用了网络的“策略部分”（即 Actor 部分）来计算策略损失函数。<br>    如果你想要使用网络的“价值部分”（即 Critic 部分），你需要定义相应的价值损失函数并将其加入最终的总损失函数之中。</p></div><div class="code"><pre><code id="code_15" name="py_code">    total_loss = loss.policy_loss - entropy_weight * loss.entropy_loss
</code></pre></div></div><div class="section" id="section-16"><div class="docs doc-strings"><p>    PyTorch 的反向传播及参数更新。</p></div><div class="code"><pre><code id="code_16" name="py_code">    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()
    print('mapg_training_operator is ok')

</code></pre></div></div><div class="section" id="section-17"><div class="docs doc-strings"><p>    <b>maac_training_opeator 功能概述</b><br>        关于 CTDE Actor-Critic 算法的训练过程的主函数。<br>        定义一些超参数，神经网络和优化器，然后生成随机伪数据并计算相关损失函数。在实践中，训练数据应被替换为与环境互动获得的结果。<br>        最后，使用优化器更新网络参数。在本文中，网络的策略部分指的是 Actor，而价值部分网络则指的是 Critic。</p></div><div class="code"><pre><code id="code_17" name="py_code">def maac_training_opeator() -> None:</code></pre></div></div><div class="section" id="section-19"><div class="docs doc-strings"><p>    设置必要的超参数。</p></div><div class="code"><pre><code id="code_19" name="py_code">    batch_size, agent_num, local_state_shape, global_state_shape, action_shape = 4, 5, 10, 20, 6</code></pre></div></div><div class="section" id="section-20"><div class="docs doc-strings"><p>    Entropy bonus 的权重，有助于智能体进行探索。</p></div><div class="code"><pre><code id="code_20" name="py_code">    entropy_weight = 0.001</code></pre></div></div><div class="section" id="section-21"><div class="docs doc-strings"><p>    价值损失函数的权重，用于平衡损失函数值的大小。</p></div><div class="code"><pre><code id="code_21" name="py_code">    value_weight = 0.5</code></pre></div></div><div class="section" id="section-22"><div class="docs doc-strings"><p>    对未来奖励的折扣因子</p></div><div class="code"><pre><code id="code_22" name="py_code">    discount_factor = 0.99</code></pre></div></div><div class="section" id="section-23"><div class="docs doc-strings"><p>    根据运行环境设定，决定 tensor 放置于 cpu 或是 cuda 。</p></div><div class="code"><pre><code id="code_23" name="py_code">    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
</code></pre></div></div><div class="section" id="section-24"><div class="docs doc-strings"><p>    定义多智能体神经网络和优化器。</p></div><div class="code"><pre><code id="code_24" name="py_code">    model = CTDEActorCriticNetwork(agent_num, local_state_shape, global_state_shape, action_shape)
    model.to(device)</code></pre></div></div><div class="section" id="section-25"><div class="docs doc-strings"><p>    Adam 是深度强化学习中最常用的优化器。如果你想使用 weight decay，你应当使用 <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">torch.optim.AdamW</span> 优化器</p></div><div class="code"><pre><code id="code_25" name="py_code">    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
</code></pre></div></div><div class="section" id="section-26"><div class="docs doc-strings"><p>    定义对应随机生成的伪数据，其格式与真实和环境交互得到的数据相同。<br>    要注意，数据和网络应当在相同的设备上 (cpu 或 cuda)。<br>    简单起见，我们这里假定一个 batch 的数据构成了一个完整的 episode。<br>    在真实实践中，一个训练 batch 可能是多个 episode 混在一起的结果。我们常常使用 <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">done</span> 这个变量来区分不同的 episodes。</p></div><div class="code"><pre><code id="code_26" name="py_code">    local_state = torch.randn(batch_size, agent_num, local_state_shape).to(device)
    global_state = torch.randn(batch_size, global_state_shape).to(device)
    action = torch.randint(0, action_shape, (batch_size, agent_num)).to(device)
    reward = torch.randn(batch_size, agent_num).to(device)</code></pre></div></div><div class="section" id="section-27"><div class="docs doc-strings"><p>    累积回报值可以使用多种不同的方式进行计算，在这里我们使用由带折扣因子的累积奖励。<br>    你也可以使用 generalized advantage estimation (GAE), n-step 等其他方式计算该值。</p></div><div class="code"><pre><code id="code_27" name="py_code">    return_ = torch.zeros_like(reward)
    for i in reversed(range(batch_size)):
        return_[i] = reward[i] + (discount_factor * return_[i + 1] if i + 1 < batch_size else 0)
</code></pre></div></div><div class="section" id="section-28"><div class="docs doc-strings"><p>    Actor-Critic 网络前向计算。</p></div><div class="code"><pre><code id="code_28" name="py_code">    output = model(local_state, global_state)</code></pre></div></div><div class="section" id="section-29"><div class="docs doc-strings"><p>    保证 value 的形状为 $$(B, 1)$$, 这样可以使得这个张量在后续计算中可以自动广播出不同智能体的维度。</p></div><div class="code"><pre><code id="code_29" name="py_code">    value = output.value</code></pre></div></div><div class="section" id="section-30"><div class="docs doc-strings"><p>    准备用于计算策略梯度损失函数的数据。<br>    <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">detach</span> 操作可以使得在计算损失函数的梯度时， <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">value</span> 的梯度不进行反向传播。</p></div><div class="code"><pre><code id="code_30" name="py_code">    data = pg_data(output.logit, action, value.detach())</code></pre></div></div><div class="section" id="section-31"><div class="docs doc-strings"><p>    计算策略梯度损失函数。</p></div><div class="code"><pre><code id="code_31" name="py_code">    loss = pg_error(data)</code></pre></div></div><div class="section" id="section-32"><div class="docs doc-strings"><p>    计算价值损失函数。<br>    需要注意的是，由于我们对于各个智能体都使用同一个全局的状态来计算价值函数，因此总的目标价值应当是各个智能体的回报之和。</p></div><div class="code"><pre><code id="code_32" name="py_code">    value_loss = torch.nn.functional.mse_loss(value, return_.sum(dim=-1, keepdim=True))</code></pre></div></div><div class="section" id="section-33"><div class="docs doc-strings"><p>    策略损失函数、价值损失函数、熵损失函数的加权和。</p></div><div class="code"><pre><code id="code_33" name="py_code">    total_loss = loss.policy_loss + value_weight * value_loss - entropy_weight * loss.entropy_loss
</code></pre></div></div><div class="section" id="section-34"><div class="docs doc-strings"><p>    PyTorch 的反向传播及参数更新。</p></div><div class="code"><pre><code id="code_34" name="py_code">    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()
    print('maac_training_operator is ok')

</code></pre></div></div><div class="section" id="section-34"><div class="docs doc-strings"><p><i>如果读者关于本文档有任何问题和建议，可以在 GitHub 提 issue 或是直接发邮件给我们 (opendilab@pjlab.org.cn) 。</i></p></div></div></body><script type="text/javascript">
window.onload = function(){
    var codeElement = document.getElementsByName('py_code');
    var lineCount = 1;
    for (var i = 0; i < codeElement.length; i++) {
        var code = codeElement[i].innerText;
        if (code.length <= 1) {
            continue;
        }

        codeElement[i].innerHTML = "";

        var codeMirror = CodeMirror(
          codeElement[i],
          {
            value: code,
            mode: "python",
            theme: "solarized dark",
            lineNumbers: true,
            firstLineNumber: lineCount,
            readOnly: false,
            lineWrapping: true,
          }
        );
        var noNewLineCode = code.replace(/[\r\n]/g, "");
        lineCount += code.length - noNewLineCode.length + 1;
    }
};
</script></html>