<!DOCTYPE html>
<html><head><meta charset="utf-8"></meta><title>Annonated Algorithm Visualization</title><link rel="stylesheet" href="pylit.css?v=1"></link><link rel="stylesheet" href="solarized.css"></link><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" integrity="sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/" crossorigin="anonymous"></link><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js" integrity="sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);" defer="True"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.css"></link><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.js"></script><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/mode/python/python.min.js"></script></head><body><div class="section" id="section-0"><div class="docs doc-strings"><p><p><a href="index.html"><b>HOME<br></b></a></p></p><a href="https://github.com/opendilab/PPOxFamily" target="_blank"><img alt="GitHub" style="max-width:100%;" src="https://img.shields.io/github/stars/opendilab/PPOxFamily?style=social"></img></a>  <a href="https://space.bilibili.com/1112854351?spm_id_from=333.337.0.0" target="_blank"><img alt="bilibili" style="max-width:100%;" src="https://img.shields.io/badge/bilibili-video%20course-blue"></img></a>  <a href="https://twitter.com/OpenDILab" rel="nofollow" target="_blank"><img alt="twitter" style="max-width:100%;" src="https://img.shields.io/twitter/follow/opendilab?style=social"></img></a><br><a href="https://github.com/opendilab/PPOxFamily/tree/main/chapter6_marl/mappo.py" target="_blank">View code on GitHub</a><br><br>PyTorch tutorial for basic centralized training and decentralized execution (CTDE) MAPPO algorithm for multi-agent cooperation scenarios.<br>This tutorial utilizes CTDEActorCriticNetwork defined in <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">marl_network</span> and loss function defined in <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">pg</span> . The main function describes the core part of CTDE MAPPO algorithm with fake data.<br>More details about multi-agent cooperation reinforcement learning can be found in <a href="https://github.com/opendilab/PPOxFamily/blob/main/chapter6_marl/chapter6_lecture.pdf">Related Link</a>.</div></div><div class="section" id="section-2"><div class="docs doc-strings"><p>You need to copy the implementation of ppo in chapter1_overview</p></div><div class="code"><pre><code id="code_2" name="py_code">from ppo import ppo_policy_data, ppo_policy_error</code></pre></div></div><div class="section" id="section-3"><div class="docs doc-strings"><p>You need to copy the implementation of gae in chapter7_tricks</p></div><div class="code"><pre><code id="code_3" name="py_code">from gae import gae

</code></pre></div></div><div class="section" id="section-4"><div class="docs doc-strings"><p>    <b>Overview</b><br>        The main function about the training process of CTDE PPO algorithm.<br>        Define some hyper-parameters, the neural network and optimizer, then generate fake data and calculate the actor-critic loss.<br>        Finally, update the network parameters with optimizer. In practice, the training data should be<br>        replaced by the results getting from the interacting with the environment.<br>        BTW, policy network means actor and value network indicates critic in this file.</p></div><div class="code"><pre><code id="code_4" name="py_code">def mappo_training_opeator() -> None:</code></pre></div></div><div class="section" id="section-6"><div class="docs doc-strings"><p>    Set necessary hyper-parameters.</p></div><div class="code"><pre><code id="code_6" name="py_code">    batch_size, agent_num, local_state_shape, agent_specific_global_state_shape, action_shape = 4, 5, 10, 25, 6</code></pre></div></div><div class="section" id="section-7"><div class="docs doc-strings"><p>    Entropy bonus weight, which is beneficial to exploration.</p></div><div class="code"><pre><code id="code_7" name="py_code">    entropy_weight = 0.001</code></pre></div></div><div class="section" id="section-8"><div class="docs doc-strings"><p>    Value loss weight, which aims to balance the loss scale.</p></div><div class="code"><pre><code id="code_8" name="py_code">    value_weight = 0.5</code></pre></div></div><div class="section" id="section-9"><div class="docs doc-strings"><p>    Discount factor for future reward.</p></div><div class="code"><pre><code id="code_9" name="py_code">    discount_factor = 0.99</code></pre></div></div><div class="section" id="section-10"><div class="docs doc-strings"><p>    Set the tensor device to cuda or cpu according to the runtime environment.</p></div><div class="code"><pre><code id="code_10" name="py_code">    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
</code></pre></div></div><div class="section" id="section-11"><div class="docs doc-strings"><p>    Define the multi-agent neural network and optimizer.</p></div><div class="code"><pre><code id="code_11" name="py_code">    model = CTDEActorCriticNetwork(agent_num, local_state_shape, agent_specific_global_state_shape, action_shape)
    model.to(device)</code></pre></div></div><div class="section" id="section-12"><div class="docs doc-strings"><p>    Adam is the most commonly used optimizer in deep reinforcement learning. If you want to add<br>    weight decay mechanism, you should use <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">torch.optim.AdamW</span> .</p></div><div class="code"><pre><code id="code_12" name="py_code">    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
</code></pre></div></div><div class="section" id="section-13"><div class="docs doc-strings"><p>    Define the corresponding fake data following the same data format of the interacting with the environment.<br>    Note that the data should keep the same device with the network.<br>    For simplicity, we regard the whole batch data as a entire episode.<br>    In practice, the training batch is the combination of multiple episodes. We often use<br>    <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">done</span> variable to distinguish the different episodes.</p></div><div class="code"><pre><code id="code_13" name="py_code">    local_state = torch.randn(batch_size, agent_num, local_state_shape).to(device)
    agent_specific_global_state = torch.randn(batch_size, agent_num, agent_specific_global_state_shape).to(device)
    logit_old = torch.randn(batch_size, agent_num, action_shape).to(device)
    value_old = torch.randn(batch_size, agent_num).to(device)
    done = torch.zeros(batch_size).to(device)
    done[-1] = 1
    action = torch.randint(0, action_shape, (batch_size, agent_num)).to(device)
    reward = torch.randn(batch_size, agent_num).to(device)</code></pre></div></div><div class="section" id="section-14"><div class="docs doc-strings"><p>    Return_ can be computed with different methods. Here we use the discounted cumulative sum of the reward.<br>    You can also use the generalized advantage estimation (GAE) method, n-step return method, etc.</p></div><div class="code"><pre><code id="code_14" name="py_code">    return_ = torch.zeros_like(reward)
    for i in reversed(range(batch_size)):
        return_[i] = reward[i] + (discount_factor * return_[i + 1] if i + 1 < batch_size else 0)
</code></pre></div></div><div class="section" id="section-15"><div class="docs doc-strings"><p>    Actor-critic network forward propagation.</p></div><div class="code"><pre><code id="code_15" name="py_code">    output = model(local_state, agent_specific_global_state)</code></pre></div></div><div class="section" id="section-16"><div class="docs doc-strings"><p>    <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">squeeze</span> operation transforms shape from $$(B, A, 1)$$ to $$(B, A)$$.</p></div><div class="code"><pre><code id="code_16" name="py_code">    value = output.value.squeeze(-1)</code></pre></div></div><div class="section" id="section-17"><div class="docs doc-strings"><p>    Use generalized advantage estimation (GAE) method to calculate the advantage.<br>    Advantage is a kind of "weight" for policy loss, therefore it is wrapperd in <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">torch.no_grad()</span> .<br>    <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">done</span> is the terminal flag of the episode. <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">traj_flag</span> is the flag of the trajectory.<br>    Here we regard the whole batch data as a entire episode, so <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">done</span> and <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">traj_flag</span> are the same.</p></div><div class="code"><pre><code id="code_17" name="py_code">    with torch.no_grad():
        traj_flag = done
        gae_data = (value, value_old, reward, done, traj_flag)
        adv = gae(gae_data, discount_factor, 0.95)</code></pre></div></div><div class="section" id="section-18"><div class="docs doc-strings"><p>    Prepare the data for PPO policy loss calculation.</p></div><div class="code"><pre><code id="code_18" name="py_code">    data = ppo_policy_data(output.logit, logit_old, action, adv, None)</code></pre></div></div><div class="section" id="section-19"><div class="docs doc-strings"><p>    Calculate the PPO policy loss.</p></div><div class="code"><pre><code id="code_19" name="py_code">    loss, info = ppo_policy_error(data)</code></pre></div></div><div class="section" id="section-20"><div class="docs doc-strings"><p>    Calculate the value loss.</p></div><div class="code"><pre><code id="code_20" name="py_code">    value_loss = torch.nn.functional.mse_loss(value, return_)</code></pre></div></div><div class="section" id="section-21"><div class="docs doc-strings"><p>    Weighted sum of policy loss, value loss and entropy loss.</p></div><div class="code"><pre><code id="code_21" name="py_code">    total_loss = loss.policy_loss + value_weight * value_loss - entropy_weight * loss.entropy_loss
</code></pre></div></div><div class="section" id="section-22"><div class="docs doc-strings"><p>    PyTorch loss back propagation and optimizer update.</p></div><div class="code"><pre><code id="code_22" name="py_code">    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()</code></pre></div></div><div class="section" id="section-23"><div class="docs doc-strings"><p>    Logging the training information.</p></div><div class="code"><pre><code id="code_23" name="py_code">    print(
        'total_loss: {:.4f}, policy_loss: {:.4f}, value_loss: {:.4f}, entropy_loss: {:.4f}'.format(
            total_loss, loss.policy_loss, value_loss, loss.entropy_loss
        )
    )
    print('approximate_kl_divergence: {:.4f}, clip_fraction: {:.4f}'.format(info.approx_kl, info.clipfrac))
    print('mappo_training_opeator is ok')

</code></pre></div></div><div class="section" id="section-23"><div class="docs doc-strings"><p><i>If you have any questions or advices about this documation, you can raise issues in GitHub (https://github.com/opendilab/PPOxFamily) or email us (opendilab@pjlab.org.cn).</i></p></div></div></body><script type="text/javascript">
window.onload = function(){
    var codeElement = document.getElementsByName('py_code');
    var lineCount = 1;
    for (var i = 0; i < codeElement.length; i++) {
        var code = codeElement[i].innerText;
        if (code.length <= 1) {
            continue;
        }

        codeElement[i].innerHTML = "";

        var codeMirror = CodeMirror(
          codeElement[i],
          {
            value: code,
            mode: "python",
            theme: "solarized dark",
            lineNumbers: true,
            firstLineNumber: lineCount,
            readOnly: false,
            lineWrapping: true,
          }
        );
        var noNewLineCode = code.replace(/[\r\n]/g, "");
        lineCount += code.length - noNewLineCode.length + 1;
    }
};
</script></html>