<!DOCTYPE html>
<html><head><meta charset="utf-8"></meta><title>Annonated Algorithm Visualization</title><link rel="stylesheet" href="pylit.css?v=1"></link><link rel="stylesheet" href="solarized.css"></link><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" integrity="sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/" crossorigin="anonymous"></link><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js" integrity="sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);" defer="True"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.css"></link><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.js"></script><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/mode/python/python.min.js"></script></head><body><div class="section" id="section-0"><div class="docs doc-strings"><p><p><a href="index.html"><b>HOME<br></b></a></p></p><a href="https://github.com/opendilab/PPOxFamily" target="_blank"><img alt="GitHub" style="max-width:100%;" src="https://img.shields.io/github/stars/opendilab/PPOxFamily?style=social"></img></a>  <a href="https://space.bilibili.com/1112854351?spm_id_from=333.337.0.0" target="_blank"><img alt="bilibili" style="max-width:100%;" src="https://img.shields.io/badge/bilibili-video%20course-blue"></img></a>  <a href="https://twitter.com/OpenDILab" rel="nofollow" target="_blank"><img alt="twitter" style="max-width:100%;" src="https://img.shields.io/twitter/follow/opendilab?style=social"></img></a><br><a href="https://github.com/opendilab/PPOxFamily/tree/main/chapter7_tricks/grad_ignore.py" target="_blank">View code on GitHub</a><br><br>PyTorch implementation of <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">grad_ignore_norm_</span> and <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">grad_ignore_value_</span> .</div></div><div class="section" id="section-1"><div class="docs doc-strings"><p>    <b>Overview</b><br>        Implementation of grad_ignore_norm <a href="https://github.com/opendilab/DI-engine/blob/2ab7c44a64329fb90fa877e6070bc76bb6fdb31e/ding/torch_utils/optimizer_helper.py#L56">Related Link</a><br>        Different from <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">grad_clip_norm</span> , <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">grad_ignore_norm</span> <b>ignore</b> those gradients that have a norm exceeds the specified threshold, instead of cliping their norm to the threshold.</p></div><div class="code"><pre><code id="code_1" name="py_code">import torch
from torch._six import inf
from typing import Union, Iterable

_tensor_or_tensors = Union[torch.Tensor, Iterable[torch.Tensor]]


def grad_ignore_norm_(parameters: _tensor_or_tensors, max_norm: float, norm_type: float = 2.0) -> torch.Tensor:</code></pre></div></div><div class="section" id="section-3"><div class="docs doc-strings"><p>    Save the parameters with non-empty gradient into a list.</p></div><div class="code"><pre><code id="code_3" name="py_code">    if isinstance(parameters, torch.Tensor):
        parameters = [parameters]
    parameters = list(filter(lambda p: p.grad is not None, parameters))</code></pre></div></div><div class="section" id="section-4"><div class="docs doc-strings"><p>    Convert max_norm and norm_type to float.</p></div><div class="code"><pre><code id="code_4" name="py_code">    max_norm = float(max_norm)
    norm_type = float(norm_type)
    device = parameters[0].grad.device</code></pre></div></div><div class="section" id="section-5"><div class="docs doc-strings"><p>    The max norm of gradient: $$\mathrm{total\_norm}^{\infty} = \max_{\theta_i\in \Theta} |\mathrm{grad}(\theta_i)|$$</p></div><div class="code"><pre><code id="code_5" name="py_code">    if norm_type == inf:
        norms = [p.grad.detach().abs().max().to(device) for p in parameters]
        total_norm = norms[0] if len(norms) == 1 else torch.max(torch.stack(norms))</code></pre></div></div><div class="section" id="section-6"><div class="docs doc-strings"><p>    The p-norm of gradient: $$\begin{split}\mathrm{total\_norm} &amp;= (\sum_{\theta\in\Theta}((\sum_{\theta_i}\mathrm{grad}(\theta_i)^p)^\frac{1}{p})^p)^\frac{1}{p}\\&amp;=(\sum_{\theta\in\Theta}(\sum_{\theta_i}\mathrm{grad}(\theta_i)^p))^\frac{1}{p}\end{split}$$</p></div><div class="code"><pre><code id="code_6" name="py_code">    else:
        total_norm = torch.norm(
            torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type
        )</code></pre></div></div><div class="section" id="section-7"><div class="docs doc-strings"><p>    The clip coefficient (the 1e-6 is used to avoid zero in the denominator): $$\mathrm{clip\_coef} = \frac{\mathrm{max\_norm}}{\mathrm{total\_norm}}$$</p></div><div class="code"><pre><code id="code_7" name="py_code">    clip_coef = max_norm / (total_norm + 1e-6)</code></pre></div></div><div class="section" id="section-8"><div class="docs doc-strings"><p>    If total_norm > max_norm, all the gradient is clipped to zero.</p></div><div class="code"><pre><code id="code_8" name="py_code">    if clip_coef < 1:
        for p in parameters:
            p.grad.zero_()
    return total_norm

</code></pre></div></div><div class="section" id="section-9"><div class="docs doc-strings"><p>    <b>Overview</b><br>        Implementation of grad_ignore_value <a href="https://github.com/opendilab/DI-engine/blob/2ab7c44a64329fb90fa877e6070bc76bb6fdb31e/ding/torch_utils/optimizer_helper.py#L77">Related Link</a><br>        Different from <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">grad_clip_value</span> , <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">grad_ignore_value</span> <b>ignore</b> all the gradients when any of them exceeds the specified threshold, instead of cliping them to the threshold.</p></div><div class="code"><pre><code id="code_9" name="py_code">def grad_ignore_value_(parameters: _tensor_or_tensors, clip_value: float) -> None:</code></pre></div></div><div class="section" id="section-11"><div class="docs doc-strings"><p>    Save the parameters with non-empty gradient into a list.</p></div><div class="code"><pre><code id="code_11" name="py_code">    if isinstance(parameters, torch.Tensor):
        parameters = [parameters]
    parameters = list(filter(lambda p: p.grad is not None, parameters))</code></pre></div></div><div class="section" id="section-12"><div class="docs doc-strings"><p>    Convert clip_value to float.</p></div><div class="code"><pre><code id="code_12" name="py_code">    clip_value = float(clip_value)
    flag = False</code></pre></div></div><div class="section" id="section-13"><div class="docs doc-strings"><p>    Check if there is any gradient that exceeds the clip_value.</p></div><div class="code"><pre><code id="code_13" name="py_code">    for p in parameters:
        val = p.grad.data.abs().max()
        if val >= clip_value:
            flag = True
            break</code></pre></div></div><div class="section" id="section-14"><div class="docs doc-strings"><p>    If there exists a gradient that exceeds the clip_value, then clip all the gradients to zero.</p></div><div class="code"><pre><code id="code_14" name="py_code">    if flag:
        for p in parameters:
            p.grad.data.zero_()

</code></pre></div></div><div class="section" id="section-15"><div class="docs doc-strings"><p>    <b>Overview</b><br>        Test function of grad ignore norm.</p></div><div class="code"><pre><code id="code_15" name="py_code">def test_grad_ignore_norm_():</code></pre></div></div><div class="section" id="section-17"><div class="docs doc-strings"><p>    batch size=4, action=32</p></div><div class="code"><pre><code id="code_17" name="py_code">    B, N = 4, 32</code></pre></div></div><div class="section" id="section-18"><div class="docs doc-strings"><p>    Generate logit and label.</p></div><div class="code"><pre><code id="code_18" name="py_code">    logit = torch.randn(B, N).requires_grad_(True)
    label = torch.randn(B, N)</code></pre></div></div><div class="section" id="section-19"><div class="docs doc-strings"><p>    Define criterion and compute loss.</p></div><div class="code"><pre><code id="code_19" name="py_code">    criterion = torch.nn.MSELoss()
    output = criterion(logit, label)</code></pre></div></div><div class="section" id="section-20"><div class="docs doc-strings"><p>    Loss backward and compute gradients.</p></div><div class="code"><pre><code id="code_20" name="py_code">    output.backward()</code></pre></div></div><div class="section" id="section-21"><div class="docs doc-strings"><p>    Set a gradient that exceeds the threshold.</p></div><div class="code"><pre><code id="code_21" name="py_code">    logit.grad[0] = 0.5</code></pre></div></div><div class="section" id="section-22"><div class="docs doc-strings"><p>    Clip the gradient.</p></div><div class="code"><pre><code id="code_22" name="py_code">    grad_ignore_norm(logit, 0.5, 2)</code></pre></div></div><div class="section" id="section-23"><div class="docs doc-strings"><p>    Assert that all gradients are clipped to zero.</p></div><div class="code"><pre><code id="code_23" name="py_code">    assert isinstance(logit.grad, torch.Tensor)
    for g in logit.grad:
        assert (g == 0).all()

</code></pre></div></div><div class="section" id="section-24"><div class="docs doc-strings"><p>    <b>Overview</b><br>        Test function of grad ignore clip.</p></div><div class="code"><pre><code id="code_24" name="py_code">def test_grad_ignore_value_():</code></pre></div></div><div class="section" id="section-26"><div class="docs doc-strings"><p>    batch size=4, action=32</p></div><div class="code"><pre><code id="code_26" name="py_code">    B, N = 4, 32</code></pre></div></div><div class="section" id="section-27"><div class="docs doc-strings"><p>    Set clip_value as 0.5.</p></div><div class="code"><pre><code id="code_27" name="py_code">    clip_value = 0.5</code></pre></div></div><div class="section" id="section-28"><div class="docs doc-strings"><p>    Generate logit and label.</p></div><div class="code"><pre><code id="code_28" name="py_code">    logit = torch.randn(B, N).requires_grad_(True)
    label = torch.randn(B, N)</code></pre></div></div><div class="section" id="section-29"><div class="docs doc-strings"><p>    Define criterion and compute loss.</p></div><div class="code"><pre><code id="code_29" name="py_code">    criterion = torch.nn.MSELoss()
    output = criterion(logit, label)</code></pre></div></div><div class="section" id="section-30"><div class="docs doc-strings"><p>    Loss backward and compute gradients.</p></div><div class="code"><pre><code id="code_30" name="py_code">    output.backward()</code></pre></div></div><div class="section" id="section-31"><div class="docs doc-strings"><p>    Set a gradient that exceeds the threshold.</p></div><div class="code"><pre><code id="code_31" name="py_code">    logit.grad[0] = 0.6</code></pre></div></div><div class="section" id="section-32"><div class="docs doc-strings"><p>    Clip the gradient</p></div><div class="code"><pre><code id="code_32" name="py_code">    grad_ignore_value(logit, clip_value)</code></pre></div></div><div class="section" id="section-33"><div class="docs doc-strings"><p>    Assert that all gradients are clipped to zero.</p></div><div class="code"><pre><code id="code_33" name="py_code">    assert isinstance(logit.grad, torch.Tensor)
    for g in logit.grad:
        assert (g == 0).all()

</code></pre></div></div><div class="section" id="section-33"><div class="docs doc-strings"><p><i>If you have any questions or advices about this documation, you can raise issues in GitHub (https://github.com/opendilab/PPOxFamily) or email us (opendilab@pjlab.org.cn).</i></p></div></div></body><script type="text/javascript">
window.onload = function(){
    var codeElement = document.getElementsByName('py_code');
    var lineCount = 1;
    for (var i = 0; i < codeElement.length; i++) {
        var code = codeElement[i].innerText;
        if (code.length <= 1) {
            continue;
        }

        codeElement[i].innerHTML = "";

        var codeMirror = CodeMirror(
          codeElement[i],
          {
            value: code,
            mode: "python",
            theme: "solarized dark",
            lineNumbers: true,
            firstLineNumber: lineCount,
            readOnly: false,
            lineWrapping: true,
          }
        );
        var noNewLineCode = code.replace(/[\r\n]/g, "");
        lineCount += code.length - noNewLineCode.length + 1;
    }
};
</script></html>