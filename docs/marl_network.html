<!DOCTYPE html>
<html><head><meta charset="utf-8"></meta><title>Annonated Algorithm Visualization</title><link rel="stylesheet" href="pylit.css?v=1"></link><link rel="stylesheet" href="solarized.css"></link><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" integrity="sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/" crossorigin="anonymous"></link><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js" integrity="sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);" defer="True"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.css"></link><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.js"></script><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/mode/python/python.min.js"></script></head><body><div class="section" id="section-0"><div class="docs doc-strings"><p><p><a href="index.html"><b>HOME<br></b></a></p></p><a href="https://github.com/opendilab/PPOxFamily" target="_blank"><img alt="GitHub" style="max-width:100%;" src="https://img.shields.io/github/stars/opendilab/PPOxFamily?style=social"></img></a>  <a href="https://space.bilibili.com/1112854351?spm_id_from=333.337.0.0" target="_blank"><img alt="bilibili" style="max-width:100%;" src="https://img.shields.io/badge/bilibili-video%20course-blue"></img></a>  <a href="https://twitter.com/OpenDILab" rel="nofollow" target="_blank"><img alt="twitter" style="max-width:100%;" src="https://img.shields.io/twitter/follow/opendilab?style=social"></img></a><br><a href="https://github.com/opendilab/PPOxFamily/tree/main/chapter6_marl/marl_network.py" target="_blank">View code on GitHub</a><br><br>PyTorch tutorial for neural networks in multi-agent cooperation scenarios, including independent actor-critic network (w or w/o shared parameters)<br>and centralized training critic network with decentralized execution (CTDE) actor-critic network. All the examples are based on the discrete action space.<br><br>This tutorial is mainly composed of three parts, you can learn from these parts in order or just jump to the part you are interested in:<br>  - Shared actor-critic network for independent agents<br>  - Independent actor-critic network for independent agents<br>  - CTDE actor-critic network for cooperation agents<br>More details about multi-agent cooperation reinforcement learning can be found in <a href="https://github.com/opendilab/PPOxFamily/blob/main/chapter6_marl/chapter6_lecture.pdf">Related Link</a>.</div></div><div class="section" id="section-1"><div class="docs doc-strings"><p>        <b>Overview</b><br>            The definition of basic actor-critic network in policy gradient algorithms (e.g. PG/A2C/PPO),<br>            which is mainly composed of three parts: encoder, policy head and value head.</p></div><div class="code"><pre><code id="code_1" name="py_code">import torch
import torch.nn as nn
import treetensor.torch as ttorch


class ActorCriticNetwork(nn.Module):

    def __init__(self, obs_shape: int, action_shape: int) -> None:</code></pre></div></div><div class="section" id="section-3"><div class="docs doc-strings"><p>        PyTorch necessary requirements for extending <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">nn.Module</span> . Our network should also subclass this class.</p></div><div class="code"><pre><code id="code_3" name="py_code">        super(ActorCriticNetwork, self).__init__()</code></pre></div></div><div class="section" id="section-4"><div class="docs doc-strings"><p>        Define encoder module, which maps raw local state of each agent into embedding vector.<br>        It could be different for various state, such as Convolution Neural Network (CNN) for image state and Multilayer perceptron (MLP) for vector state, respectively.<br>        Here we use two-layer MLP for vector state, i.e.<br>        $$y = max(W_2 max(W_1 x+b_1, 0) + b_2, 0)$$</p></div><div class="code"><pre><code id="code_4" name="py_code">        self.encoder = nn.Sequential(
            nn.Linear(obs_shape, 32),
            nn.ReLU(),
            nn.Linear(32, 64),
            nn.ReLU(),
        )</code></pre></div></div><div class="section" id="section-5"><div class="docs doc-strings"><p>        Define discrete action logit output network, just one-layer FC.</p></div><div class="code"><pre><code id="code_5" name="py_code">        self.policy_head = nn.Linear(64, action_shape)</code></pre></div></div><div class="section" id="section-6"><div class="docs doc-strings"><p>        Define scalar value output network.</p></div><div class="code"><pre><code id="code_6" name="py_code">        self.value_head = nn.Linear(64, 1)
</code></pre></div></div><div class="section" id="section-7"><div class="docs doc-strings"><p>        <b>Overview</b><br>            The computation graph of actor-critic network in discrete action space.</p></div><div class="code"><pre><code id="code_7" name="py_code">    def forward(self, local_obs: torch.Tensor) -> ttorch.Tensor:</code></pre></div></div><div class="section" id="section-9"><div class="docs doc-strings"><p>        Transform original local obs into embedding vector, i.e. $$(B, A, *) -> (B, A, N)$$<br>        Some network layers in PyTorch like <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">nn.Linear</span> can deal with any number of prefix dimensions, so we can just use it to process the whole multi-agent batch.</p></div><div class="code"><pre><code id="code_9" name="py_code">        x = self.encoder(local_obs)</code></pre></div></div><div class="section" id="section-10"><div class="docs doc-strings"><p>        Calculate logit for each possible discrete action choices, i.e. $$(B, A, N) -> (B, A, M)$$</p></div><div class="code"><pre><code id="code_10" name="py_code">        logit = self.policy_head(x)</code></pre></div></div><div class="section" id="section-11"><div class="docs doc-strings"><p>        Calculate value for each sample and agent, i.e. $$(B, A, N) -> (B, A, 1)$$</p></div><div class="code"><pre><code id="code_11" name="py_code">        value = self.value_head(x)</code></pre></div></div><div class="section" id="section-12"><div class="docs doc-strings"><p>        Return the final result by treetensor format.</p></div><div class="code"><pre><code id="code_12" name="py_code">        return ttorch.as_tensor({
            'logit': logit,
            'value': value,
        })

</code></pre></div></div><div class="section" id="section-13"><div class="docs doc-strings"><p>        <b>Overview</b><br>            The definition of shared parameters actor-critic network in policy gradient algorithms for multi-agent scenarios.<br>            Each agent shares the same parameters in the network so that they can be processed as a batch in parallel.</p></div><div class="code"><pre><code id="code_13" name="py_code">class SharedActorCriticNetwork(nn.Module):

    def __init__(self, agent_num: int, obs_shape: int, action_shape: int) -> None:</code></pre></div></div><div class="section" id="section-15"><div class="docs doc-strings"><p>        PyTorch necessary requirements for extending <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">nn.Module</span> . Our network should also subclass this class.</p></div><div class="code"><pre><code id="code_15" name="py_code">        super(SharedActorCriticNetwork, self).__init__()</code></pre></div></div><div class="section" id="section-16"><div class="docs doc-strings"><p>        The shape of forward input is $$(B, A, O)$$.</p></div><div class="code"><pre><code id="code_16" name="py_code">        self.agent_num = agent_num</code></pre></div></div><div class="section" id="section-17"><div class="docs doc-strings"><p>        Define a shared actor-critic network used for all the agents.</p></div><div class="code"><pre><code id="code_17" name="py_code">        self.actor_critic_network = ActorCriticNetwork(obs_shape, action_shape)
</code></pre></div></div><div class="section" id="section-18"><div class="docs doc-strings"><p>        <b>Overview</b><br>            The computation graph of shared parameters actor-critic network, processing all agents' <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">local_obs</span> and output<br>            corresponding policy logit and value respectively.</p></div><div class="code"><pre><code id="code_18" name="py_code">    def forward(self, local_obs: torch.Tensor) -> ttorch.Tensor:</code></pre></div></div><div class="section" id="section-20"><div class="docs doc-strings"><p>        Call the actor_critic_network in parallel.</p></div><div class="code"><pre><code id="code_20" name="py_code">        return self.actor_critic_network(local_obs)

</code></pre></div></div><div class="section" id="section-21"><div class="docs doc-strings"><p>        <b>Overview</b><br>            The definition of independent actor-critic network in policy gradient algorithms for multi-agent scenarios.<br>            Each agent owns an independent actor-critic network with its own parameters.</p></div><div class="code"><pre><code id="code_21" name="py_code">class IndependentActorCriticNetwork(nn.Module):

    def __init__(self, agent_num: int, obs_shape: int, action_shape: int) -> None:</code></pre></div></div><div class="section" id="section-23"><div class="docs doc-strings"><p>        PyTorch necessary requirements for extending <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">nn.Module</span> . Our network should also subclass this class.</p></div><div class="code"><pre><code id="code_23" name="py_code">        super(IndependentActorCriticNetwork, self).__init__()</code></pre></div></div><div class="section" id="section-24"><div class="docs doc-strings"><p>        Define <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">agent_num</span> independent actor-critic networks for each agent.<br>        To reuse some attributes of <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">nn.Module</span> , we use <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">nn.ModuleList</span> to store these networks instead of Python native list.</p></div><div class="code"><pre><code id="code_24" name="py_code">        self.agent_num = agent_num
        self.actor_critic_networks = nn.ModuleList(
            [ActorCriticNetwork(obs_shape, action_shape) for _ in range(agent_num)]
        )
</code></pre></div></div><div class="section" id="section-25"><div class="docs doc-strings"><p>        <b>Overview</b><br>            The computation graph of independent actor-critic network, serially processing each agent's<br>            <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">local_obs</span> and output the cooresponding policy logit and value respectively.</p></div><div class="code"><pre><code id="code_25" name="py_code">    def forward(self, local_obs: torch.Tensor) -> ttorch.Tensor:</code></pre></div></div><div class="section" id="section-27"><div class="docs doc-strings"><p>        Slice data, call the actor_critic_network serially, then concatenate the output.</p></div><div class="code"><pre><code id="code_27" name="py_code">        return ttorch.cat([net(local_obs[:, i:i + 1]) for i, net in enumerate(self.actor_critic_networks)], dim=1)

</code></pre></div></div><div class="section" id="section-28"><div class="docs doc-strings"><p>        <b>Overview</b><br>            The definition of centralized training decentralized execution (CTDE) actor-critic network in policy gradient algorithms for multi-agent scenarios.<br>            Each agent shares the same parameters in the network so that they can be processed as a batch in parallel.<br>            The input of value network is <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">global_obs</span> while the input of policy network is <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">local_obs</span> .<br>            Global information used in value network can provide more guidance for the training of policy network.<br>            Local information used in policy network can make the policy network more robust to the decentralized execution.</p></div><div class="code"><pre><code id="code_28" name="py_code">class CTDEActorCriticNetwork(nn.Module):

    def __init__(self, agent_num: int, local_obs_shape: int, global_obs_shape: int, action_shape: int) -> None:</code></pre></div></div><div class="section" id="section-30"><div class="docs doc-strings"><p>        PyTorch necessary requirements for extending <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">nn.Module</span> . Our network should also subclass this class.</p></div><div class="code"><pre><code id="code_30" name="py_code">        super(CTDEActorCriticNetwork, self).__init__()</code></pre></div></div><div class="section" id="section-31"><div class="docs doc-strings"><p>        Define local and global encoder respectively.</p></div><div class="code"><pre><code id="code_31" name="py_code">        self.agent_num = agent_num
        self.local_encoder = nn.Sequential(
            nn.Linear(local_obs_shape, 32),
            nn.ReLU(),
            nn.Linear(32, 64),
            nn.ReLU(),
        )
        self.global_encoder = nn.Sequential(
            nn.Linear(global_obs_shape, 32),
            nn.ReLU(),
            nn.Linear(32, 64),
            nn.ReLU(),
        )</code></pre></div></div><div class="section" id="section-32"><div class="docs doc-strings"><p>        Define discrete action logit output network, just one-layer FC.</p></div><div class="code"><pre><code id="code_32" name="py_code">        self.policy_head = nn.Linear(64, action_shape)</code></pre></div></div><div class="section" id="section-33"><div class="docs doc-strings"><p>        Define scalar value output network.</p></div><div class="code"><pre><code id="code_33" name="py_code">        self.value_head = nn.Linear(64, 1)
</code></pre></div></div><div class="section" id="section-34"><div class="docs doc-strings"><p>        <b>Overview</b><br>            The computation graph of CTDE actor-critic network, processing all agents' <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">local_obs</span> and <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">global_obs</span> and output<br>            corresponding policy logit and value in parallel.<br>            There are two possible designs for <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">global_obs</span> : The former is a shared global state for all agents, i.e. $$(B, S)$$.<br>            Tha latter is a kind of agent-specific global state, i.e. $$(B, A, S')$$.<br>            For more details, you can refer to <a href="https://di-engine-docs.readthedocs.io/zh_CN/latest/04_best_practice/marl_zh.html#id10">Related Link</a>.</p></div><div class="code"><pre><code id="code_34" name="py_code">    def forward(self, local_obs: torch.Tensor, global_obs: torch.Tensor) -> ttorch.Tensor:</code></pre></div></div><div class="section" id="section-36"><div class="docs doc-strings"><p>        Call policy network with local obs and critic network with global obs respectively.</p></div><div class="code"><pre><code id="code_36" name="py_code">        policy = self.policy_head(self.local_encoder(local_obs))
        value = self.value_head(self.global_encoder(global_obs))
        return ttorch.as_tensor({
            'logit': policy,
            'value': value,
        })

</code></pre></div></div><div class="section" id="section-37"><div class="docs doc-strings"><p>    <b>Overview</b><br>        The function of testing shared parameters actor-critic network. Construct a network and pass a batch of data to it.<br>        Then validate the shape of different parts of output.</p></div><div class="code"><pre><code id="code_37" name="py_code">def test_shared_ac_network() -> None:</code></pre></div></div><div class="section" id="section-39"><div class="docs doc-strings"><p>    Set batch size, agent number, observation shape and action shape.</p></div><div class="code"><pre><code id="code_39" name="py_code">    batch_size = 4
    agent_num = 3
    obs_shape = 10
    action_shape = 5</code></pre></div></div><div class="section" id="section-40"><div class="docs doc-strings"><p>    Define a shared actor-critic network.</p></div><div class="code"><pre><code id="code_40" name="py_code">    network = SharedActorCriticNetwork(agent_num, obs_shape, action_shape)</code></pre></div></div><div class="section" id="section-41"><div class="docs doc-strings"><p>    Generate a batch of local obs data for all agents from the standard normal distribution.</p></div><div class="code"><pre><code id="code_41" name="py_code">    local_obs = torch.randn(batch_size, agent_num, obs_shape)</code></pre></div></div><div class="section" id="section-42"><div class="docs doc-strings"><p>    Actor-critic network forward procedure, pass the local obs data to the network and get the output.</p></div><div class="code"><pre><code id="code_42" name="py_code">    result = network(local_obs)</code></pre></div></div><div class="section" id="section-43"><div class="docs doc-strings"><p>    Validate the shape of output.</p></div><div class="code"><pre><code id="code_43" name="py_code">    assert result['logit'].shape == (batch_size, agent_num, action_shape)
    assert result['value'].shape == (batch_size, agent_num, 1)

</code></pre></div></div><div class="section" id="section-44"><div class="docs doc-strings"><p>    <b>Overview</b><br>        The function of testing independent actor-critic network. Construct a network and pass a batch of data to it.<br>        Then validate the shape of different parts of output.</p></div><div class="code"><pre><code id="code_44" name="py_code">def test_independent_ac_network() -> None:</code></pre></div></div><div class="section" id="section-46"><div class="docs doc-strings"><p>    Set batch size, agent number, observation shape and action shape.</p></div><div class="code"><pre><code id="code_46" name="py_code">    batch_size = 4
    agent_num = 3
    obs_shape = 10
    action_shape = 5</code></pre></div></div><div class="section" id="section-47"><div class="docs doc-strings"><p>    Define a independent actor-critic network.</p></div><div class="code"><pre><code id="code_47" name="py_code">    network = IndependentActorCriticNetwork(agent_num, obs_shape, action_shape)</code></pre></div></div><div class="section" id="section-48"><div class="docs doc-strings"><p>    Generate a batch of local obs data for all agents from the standard normal distribution.</p></div><div class="code"><pre><code id="code_48" name="py_code">    local_obs = torch.randn(batch_size, agent_num, obs_shape)</code></pre></div></div><div class="section" id="section-49"><div class="docs doc-strings"><p>    Actor-critic network forward procedure, pass the local obs data to the network and get the output.</p></div><div class="code"><pre><code id="code_49" name="py_code">    result = network(local_obs)</code></pre></div></div><div class="section" id="section-50"><div class="docs doc-strings"><p>    Validate the shape of output.</p></div><div class="code"><pre><code id="code_50" name="py_code">    assert result['logit'].shape == (batch_size, agent_num, action_shape)
    assert result['value'].shape == (batch_size, agent_num, 1)

</code></pre></div></div><div class="section" id="section-51"><div class="docs doc-strings"><p>    <b>Overview</b><br>        The function of testing CTDE actor-critic network. Construct a network and pass a batch of data to it.<br>        Then validate the shape of different parts of output.</p></div><div class="code"><pre><code id="code_51" name="py_code">def test_ctde_ac_network() -> None:</code></pre></div></div><div class="section" id="section-53"><div class="docs doc-strings"><p>    Set batch size, agent number, observation shape and action shape.</p></div><div class="code"><pre><code id="code_53" name="py_code">    batch_size = 4
    agent_num = 3
    local_obs_shape = 10
    global_obs_shape = 20
    action_shape = 5</code></pre></div></div><div class="section" id="section-54"><div class="docs doc-strings"><p>    Test case for the shared global obs.</p></div><div class="code"><pre><code id="code_54" name="py_code">    network = CTDEActorCriticNetwork(agent_num, local_obs_shape, global_obs_shape, action_shape)
    local_obs = torch.randn(batch_size, agent_num, local_obs_shape)
    global_obs = torch.randn(batch_size, global_obs_shape)
    result = network(local_obs, global_obs)
    assert result['logit'].shape == (batch_size, agent_num, action_shape)
    assert result['value'].shape == (batch_size, 1)
</code></pre></div></div><div class="section" id="section-55"><div class="docs doc-strings"><p>    Test case for the agent-specific global obs.</p></div><div class="code"><pre><code id="code_55" name="py_code">    agent_specific_global_obs_shape = 25
    network = CTDEActorCriticNetwork(agent_num, local_obs_shape, agent_specific_global_obs_shape, action_shape)
    local_obs = torch.randn(batch_size, agent_num, local_obs_shape)
    agent_specific_global_obs = torch.randn(batch_size, agent_num, agent_specific_global_obs_shape)
    result = network(local_obs, agent_specific_global_obs)
    assert result['logit'].shape == (batch_size, agent_num, action_shape)
    assert result['value'].shape == (batch_size, agent_num, 1)

</code></pre></div></div><div class="section" id="section-55"><div class="docs doc-strings"><p><i>If you have any questions or advices about this documation, you can raise issues in GitHub (https://github.com/opendilab/PPOxFamily) or email us (opendilab@pjlab.org.cn).</i></p></div></div></body><script type="text/javascript">
window.onload = function(){
    var codeElement = document.getElementsByName('py_code');
    var lineCount = 1;
    for (var i = 0; i < codeElement.length; i++) {
        var code = codeElement[i].innerText;
        if (code.length <= 1) {
            continue;
        }

        codeElement[i].innerHTML = "";

        var codeMirror = CodeMirror(
          codeElement[i],
          {
            value: code,
            mode: "python",
            theme: "solarized dark",
            lineNumbers: true,
            firstLineNumber: lineCount,
            readOnly: false,
            lineWrapping: true,
          }
        );
        var noNewLineCode = code.replace(/[\r\n]/g, "");
        lineCount += code.length - noNewLineCode.length + 1;
    }
};
</script></html>