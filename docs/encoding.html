<!DOCTYPE html>
<html><head><meta charset="utf-8"></meta><title>Annonated Algorithm Visualization</title><link rel="stylesheet" href="pylit.css?v=1"></link><link rel="stylesheet" href="solarized.css"></link><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" integrity="sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/" crossorigin="anonymous"></link><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js" integrity="sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);" defer="True"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.css"></link><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.js"></script><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/mode/python/python.min.js"></script></head><body><div class="section" id="section-0"><div class="docs doc-strings"><p><p><a href="index.html"><b>HOME<br></b></a></p></p><a href="https://github.com/opendilab/PPOxFamily" target="_blank"><img alt="GitHub" style="max-width:100%;" src="https://img.shields.io/github/stars/opendilab/PPOxFamily?style=social"></img></a>  <a href="https://space.bilibili.com/1112854351?spm_id_from=333.337.0.0" target="_blank"><img alt="bilibili" style="max-width:100%;" src="https://img.shields.io/badge/bilibili-video%20course-blue"></img></a>  <a href="https://twitter.com/OpenDILab" rel="nofollow" target="_blank"><img alt="twitter" style="max-width:100%;" src="https://img.shields.io/twitter/follow/opendilab?style=social"></img></a><br><a href="https://github.com/opendilab/PPOxFamily/tree/main/chapter3_obs/encoding.py" target="_blank">View code on GitHub</a><br><br>We provide a method to convert a tensor to one hot encoding, using <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">torch.Tensor._scatter</span> .<br>Also, we provide examples using <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">torch.nn.Embedding</span> to implement one-hot encoding and binary encoding.<br>Internally, weight in <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">torch.nn.Embedding</span> is a M x N matrix, with M being the number of words and N being the size of each word vector.<br>It matches a word index to the corresponding embedding vector, i.e., the corresponding row in the matrix.<br>This document mainly includes:<br>- One-hot encoding implementation using <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">torch.Tensor._scatter</span> .<br>- One-hot encoding implementation using <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">torch.nn.Embedding</span> .<br>- Binary encoding implementation using <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">torch.nn.Embedding</span> .</div></div><div class="section" id="section-1"><div class="docs doc-strings"><p>    <b>Overview</b><br>        Convert a <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">torch.LongTensor</span> to one hot encoding with scatter API.<br>        This implementation can be slightly faster than <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">torch.nn.functional.one_hot</span> .</p></div><div class="code"><pre><code id="code_1" name="py_code">import torch
import torch.nn as nn


def one_hot(val: torch.LongTensor, num: int) -> torch.FloatTensor:</code></pre></div></div><div class="section" id="section-3"><div class="docs doc-strings"><p>    Remember original shape of val.</p></div><div class="code"><pre><code id="code_3" name="py_code">    old_shape = val.shape</code></pre></div></div><div class="section" id="section-4"><div class="docs doc-strings"><p>    Reshape val into 2D tensor.</p></div><div class="code"><pre><code id="code_4" name="py_code">    val_reshape = val.reshape(-1, 1)</code></pre></div></div><div class="section" id="section-5"><div class="docs doc-strings"><p>    Initialize return tensor with float32 dtype and the same device as val.</p></div><div class="code"><pre><code id="code_5" name="py_code">    ret = torch.zeros(val_reshape.shape[0], num, device=val.device)</code></pre></div></div><div class="section" id="section-6"><div class="docs doc-strings"><p>    Fill value 1 into tensor <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">ret</span> , according to the index stored in <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">val_reshape</span> . It is an inplace operation.</p></div><div class="code"><pre><code id="code_6" name="py_code">    ret.scatter_(1, val_reshape, 1)</code></pre></div></div><div class="section" id="section-7"><div class="docs doc-strings"><p>    Return the reshaped result with the same prefix shape as original shape of val.</p></div><div class="code"><pre><code id="code_7" name="py_code">    return ret.reshape(*old_shape, num)

</code></pre></div></div><div class="section" id="section-8"><div class="docs doc-strings"><p>    <b>Overview</b><br>        Implementation of one hot encoding with nn.Embedding API.</p></div><div class="code"><pre><code id="code_8" name="py_code">def get_one_hot_encoding(num: int):</code></pre></div></div><div class="section" id="section-10"><div class="docs doc-strings"><p>    Use the identity matrix as weight tensor.<br>    Use freezed embedding as fixed one-hot transformation.</p></div><div class="code"><pre><code id="code_10" name="py_code">    return nn.Embedding.from_pretrained(torch.eye(num), freeze=True, padding_idx=None)

</code></pre></div></div><div class="section" id="section-11"><div class="docs doc-strings"><p>    <b>Overview</b><br>        Implementation of binary encoding with nn.Embedding API.</p></div><div class="code"><pre><code id="code_11" name="py_code">def get_binary_encoding(bit_num: int):</code></pre></div></div><div class="section" id="section-13"><div class="docs doc-strings"><p>    Generate a matrix with shape $$2^{B} \times B $$ where B is the bit_num.<br>    Each row with index n contains the binary representation of n.</p></div><div class="code"><pre><code id="code_13" name="py_code">    location_embedding = []
    for n in range(2 ** bit_num):
        s = '0' * (bit_num - len(bin(n)[2:])) + bin(n)[2:]
        location_embedding.append(list(int(i) for i in s))
    mat = torch.FloatTensor(location_embedding)</code></pre></div></div><div class="section" id="section-14"><div class="docs doc-strings"><p>    Use the generated result as transformation..</p></div><div class="code"><pre><code id="code_14" name="py_code">    return torch.nn.Embedding.from_pretrained(mat, freeze=True, padding_idx=None)

</code></pre></div></div><div class="section" id="section-15"><div class="docs doc-strings"><p>    <b>Overview</b><br>        Test different encoding methods.</p></div><div class="code"><pre><code id="code_15" name="py_code">def test_encoding():</code></pre></div></div><div class="section" id="section-17"><div class="docs doc-strings"><p>    Test one-hot encoding with nn.Embedding and scatter, compare two float32 dtype tensor.</p></div><div class="code"><pre><code id="code_17" name="py_code">    x = torch.LongTensor([9, 0, 1, 2, 1, 3, 5])
    one_hot_enc = get_one_hot_encoding(10)
    y = one_hot_enc(x)
    y_ = one_hot(x, num=10)
    assert torch.sum(torch.abs(y - y_)) < 1e-6</code></pre></div></div><div class="section" id="section-18"><div class="docs doc-strings"><p>    Test binary encoding, compare two int64 dtype tensor.</p></div><div class="code"><pre><code id="code_18" name="py_code">    bin_enc = get_binary_encoding(2)
    x = torch.arange(4)
    y = bin_enc(x)
    ground_truth = torch.LongTensor([[0, 0], [0, 1], [1, 0], [1, 1]])
    assert torch.eq(y, ground_truth).all()

</code></pre></div></div><div class="section" id="section-18"><div class="docs doc-strings"><p><i>If you have any questions or advices about this documation, you can raise issues in GitHub (https://github.com/opendilab/PPOxFamily) or email us (opendilab@pjlab.org.cn).</i></p></div></div></body><script type="text/javascript">
window.onload = function(){
    var codeElement = document.getElementsByName('py_code');
    var lineCount = 1;
    for (var i = 0; i < codeElement.length; i++) {
        var code = codeElement[i].innerText;
        if (code.length <= 1) {
            continue;
        }

        codeElement[i].innerHTML = "";

        var codeMirror = CodeMirror(
          codeElement[i],
          {
            value: code,
            mode: "python",
            theme: "solarized dark",
            lineNumbers: true,
            firstLineNumber: lineCount,
            readOnly: false,
            lineWrapping: true,
          }
        );
        var noNewLineCode = code.replace(/[\r\n]/g, "");
        lineCount += code.length - noNewLineCode.length + 1;
    }
};
</script></html>