<!DOCTYPE html>
<html><head><meta charset="utf-8"></meta><title>Annonated Algorithm Visualization</title><link rel="stylesheet" href="pylit.css?v=1"></link><link rel="stylesheet" href="solarized.css"></link><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" integrity="sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/" crossorigin="anonymous"></link><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js" integrity="sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);" defer="True"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.css"></link><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.js"></script><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/mode/python/python.min.js"></script></head><body><div class="section" id="section-0"><div class="docs doc-strings"><p><p><a href="index.html"><b>HOME<br></b></a></p></p><a href="https://github.com/opendilab/PPOxFamily" target="_blank"><img alt="GitHub" style="max-width:100%;" src="https://img.shields.io/github/stars/opendilab/PPOxFamily?style=social"></img></a>  <a href="https://space.bilibili.com/1112854351?spm_id_from=333.337.0.0" target="_blank"><img alt="bilibili" style="max-width:100%;" src="https://img.shields.io/badge/bilibili-video%20course-blue"></img></a>  <a href="https://twitter.com/OpenDILab" rel="nofollow" target="_blank"><img alt="twitter" style="max-width:100%;" src="https://img.shields.io/twitter/follow/opendilab?style=social"></img></a><br><a href="https://github.com/opendilab/PPOxFamily/tree/main/chapter7_tricks/gae.py" target="_blank">View code on GitHub</a><br><br>Generalized Advantage Estimator (GAE)<br><br>The GAE is a technique that accurately estimates the advantage function by considering<br>both immediate and future rewards. This approach not only improves the efficacy of Proximal Policy Optimization (PPO),<br>but also enhances its stability. You can find more detailed information in this paper:<br>High-dimensional Continuous Control Using Generalized Advantage Estimation <a href="https://arxiv.org/pdf/1506.02438.pdf">Related Link</a>.</div></div><div class="section" id="section-1"><div class="docs doc-strings"><p>    <b>Overview</b><br>        Implementation of the Generalized Advantage Estimator (GAE) as proposed in arXiv:1506.02438.<br>        This function calculates the advantages, which are used to update policy parameters in reinforcement learning.<br><br>    Arguments:<br>        - data (:obj:`namedtuple`): Tuple containing trajectory data including state values, next state<br>            values, rewards, done flags, and trajectory flags. Please note that the <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">done</span> flag signals<br>            the termination of an episode, whereas the <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">traj_flag</span> indicates the completion of a trajectory, which represents a segment within an episode.<br>        - gamma (:obj:`float`): Discount factor for future rewards, should be in the range [0, 1]. Default is 0.99.<br>        - lambda_ (:obj:`float`): The decay rate for the GAE, should be in the range [0, 1]. Default is 0.97.<br>            As lambda approaches 0, it introduces bias, and as lambda approaches 1, it increases variance<br>            due to the cumulative effect of terms.<br>    Returns:<br>        - adv (:obj:`torch.FloatTensor`): The calculated advantage estimates.<br>    Shapes:<br>        - value (:obj:`torch.FloatTensor`): Size of (T, B), where T is the length of the trajectory and B is the batch size.<br>        - next_value (:obj:`torch.FloatTensor`):  Size of (T, B)<br>        - reward (:obj:`torch.FloatTensor`): Size of (T, B)<br>        - adv (:obj:`torch.FloatTensor`): Size of (T, B)</p></div><div class="code"><pre><code id="code_1" name="py_code">import torch


def gae(data: tuple, gamma: float = 0.99, lambda_: float = 0.97) -> torch.FloatTensor:</code></pre></div></div><div class="section" id="section-3"><div class="docs doc-strings"><p>    Unpack the input data.</p></div><div class="code"><pre><code id="code_3" name="py_code">    value, next_value, reward, done, traj_flag = data
</code></pre></div></div><div class="section" id="section-4"><div class="docs doc-strings"><p>    Convert the done and trajectory flags to tensor format.</p></div><div class="code"><pre><code id="code_4" name="py_code">    done = torch.tensor(done).float()
    traj_flag = torch.tensor(traj_flag).float()
</code></pre></div></div><div class="section" id="section-5"><div class="docs doc-strings"><p>    Expand <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">done</span> for possible broadcast operation in multi-agent cases</p></div><div class="code"><pre><code id="code_5" name="py_code">    if len(value.shape) == 2:
        done = done.unsqueeze(1)
</code></pre></div></div><div class="section" id="section-6"><div class="docs doc-strings"><p>    If done equals 1, it indicates the end of an episode, thus the next state value should be 0.</p></div><div class="code"><pre><code id="code_6" name="py_code">    next_value *= (1 - done)
</code></pre></div></div><div class="section" id="section-7"><div class="docs doc-strings"><p>    Calculate the temporal difference (TD) error for each time step.<br>    $$\delta_t=-V_{\phi}(s_t)+r_t+V_{\phi}(s_{t+1})$$</p></div><div class="code"><pre><code id="code_7" name="py_code">    delta = reward + gamma * next_value - value
</code></pre></div></div><div class="section" id="section-8"><div class="docs doc-strings"><p>    Set the GAE decay factor. If traj_flag equals 1, the factor will be 0. Otherwise, the factor is gamma * lambda.</p></div><div class="code"><pre><code id="code_8" name="py_code">    factor = gamma * lambda_ * (1 - traj_flag)
</code></pre></div></div><div class="section" id="section-9"><div class="docs doc-strings"><p>    Initialize the advantage tensor.</p></div><div class="code"><pre><code id="code_9" name="py_code">    adv = torch.zeros_like(value)</code></pre></div></div><div class="section" id="section-10"><div class="docs doc-strings"><p>    Calculate <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">adv</span> in a reversed sequence.<br>    Consider the definition of GAE: $$A^{GAE}_t = \sum_{i=1}\gamma^{i-1}\lambda^{i-1}\delta_{t+i-1}$$<br>    Rewrite the equation above in a recurrent form, we finally have: $$A^{GAE}_t = \delta_t + \gamma\lambda A^{GAE}_{t+1}$$</p></div><div class="code"><pre><code id="code_10" name="py_code">    gae_item = torch.zeros_like(value[0])
    for t in reversed(range(reward.shape[0])):
        gae_item = delta[t] + factor[t] * gae_item
        adv[t] = gae_item
</code></pre></div></div><div class="section" id="section-11"><div class="docs doc-strings"><p>    Return the calculated advantage estimates.</p></div><div class="code"><pre><code id="code_11" name="py_code">    return adv

</code></pre></div></div><div class="section" id="section-12"><div class="docs doc-strings"><p>    <b>Overview</b><br>        Test the GAE function with randomly generated data.</p></div><div class="code"><pre><code id="code_12" name="py_code">def test_gae() -> None:</code></pre></div></div><div class="section" id="section-14"><div class="docs doc-strings"><p>    Generate random data with trajectory length 10 and batch size 5.</p></div><div class="code"><pre><code id="code_14" name="py_code">    T, B = 10, 5
    value = torch.randn(T, B)
    next_value = torch.randn(T, B)
    reward = torch.randn(T, B)
    done = torch.randint(0, 2, (T, B)).to(torch.bool)  # Generate random boolean tensor for done flags.
    traj_flag = torch.randint(0, 2, (T, B)).to(torch.bool)  # Generate random boolean tensor for trajectory flags.
    data = (value, next_value, reward, done, traj_flag)
</code></pre></div></div><div class="section" id="section-15"><div class="docs doc-strings"><p>    Calculate GAE values.</p></div><div class="code"><pre><code id="code_15" name="py_code">    gae_value = gae(data)
</code></pre></div></div><div class="section" id="section-16"><div class="docs doc-strings"><p>    Assert that the calculated GAE values have the correct shape.</p></div><div class="code"><pre><code id="code_16" name="py_code">    assert gae_value.shape == (T, B)

</code></pre></div></div><div class="section" id="section-16"><div class="docs doc-strings"><p><i>If you have any questions or advices about this documation, you can raise issues in GitHub (https://github.com/opendilab/PPOxFamily) or email us (opendilab@pjlab.org.cn).</i></p></div></div></body><script type="text/javascript">
window.onload = function(){
    var codeElement = document.getElementsByName('py_code');
    var lineCount = 1;
    for (var i = 0; i < codeElement.length; i++) {
        var code = codeElement[i].innerText;
        if (code.length <= 1) {
            continue;
        }

        codeElement[i].innerHTML = "";

        var codeMirror = CodeMirror(
          codeElement[i],
          {
            value: code,
            mode: "python",
            theme: "solarized dark",
            lineNumbers: true,
            firstLineNumber: lineCount,
            readOnly: false,
            lineWrapping: true,
          }
        );
        var noNewLineCode = code.replace(/[\r\n]/g, "");
        lineCount += code.length - noNewLineCode.length + 1;
    }
};
</script></html>