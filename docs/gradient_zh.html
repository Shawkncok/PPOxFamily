<!DOCTYPE html>
<html><head><meta charset="utf-8"></meta><title>Annonated Algorithm Visualization</title><link rel="stylesheet" href="pylit.css?v=1"></link><link rel="stylesheet" href="solarized.css"></link><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" integrity="sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/" crossorigin="anonymous"></link><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js" integrity="sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);" defer="True"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.css"></link><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.js"></script><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/mode/python/python.min.js"></script></head><body><div class="section" id="section-0"><div class="docs doc-strings"><p><p><a href="index.html"><b>HOME<br></b></a></p></p><a href="https://github.com/opendilab/PPOxFamily" target="_blank"><img alt="GitHub" style="max-width:100%;" src="https://img.shields.io/github/stars/opendilab/PPOxFamily?style=social"></img></a>  <a href="https://space.bilibili.com/1112854351?spm_id_from=333.337.0.0" target="_blank"><img alt="bilibili" style="max-width:100%;" src="https://img.shields.io/badge/bilibili-video%20course-blue"></img></a>  <a href="https://twitter.com/OpenDILab" rel="nofollow" target="_blank"><img alt="twitter" style="max-width:100%;" src="https://img.shields.io/twitter/follow/opendilab?style=social"></img></a><br><a href="https://github.com/opendilab/PPOxFamily/tree/main/chapter3_obs/gradient_zh.py" target="_blank">View code on GitHub</a><br><br>本文档主要包括：<br>- 使用 PyTorch 定义一个可求导函数的方法<br>- 使用 Numpy 手动计算导数的方法<br>- 使用 PyTorch 自动计算导数的方法<br>本文档用于求导的函数示例是：<br>$$ c = \sum x * y + z $$<br>本文档还将介绍利用在 PyTorch 中自定义可导函数的方法。<br>通过继承 <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">torch.autograd.Function</span> <a href="https://pytorch.org/docs/stable/autograd.html?highlight=autograd+function#torch.autograd.Function">Related Link</a>，用户可以通过重写其中的前向传播、反向传播函数，自定义一个可导的函数<br>我们将以一个标准的线性函数为例进行介绍：<br>$$output = input \cdot weight^T + bias$$</div></div><div class="section" id="section-1"><div class="docs doc-strings"><p>    <b>LinearFunction 定义概述</b><br>        这是一个线性的可导函数，等价于神经网络中的线性层（全连接层）。公式是：<br>        $$output = input \cdot weight^T + bias$$</p></div><div class="code"><pre><code id="code_1" name="py_code">import numpy as np
import torch.nn as nn
import torch
from torch.autograd import Function
from copy import deepcopy


class LinearFunction(Function):</code></pre></div></div><div class="section" id="section-2"><div class="docs doc-strings"><p>        <b>forward 函数功能概述</b><br>            线性函数的前向传播计算过程。</p></div><div class="code"><pre><code id="code_2" name="py_code">
    @staticmethod
    def forward(ctx, input_, weight, bias):</code></pre></div></div><div class="section" id="section-4"><div class="docs doc-strings"><p>        保存参数，用于后续反向传播。</p></div><div class="code"><pre><code id="code_4" name="py_code">        ctx.save_for_backward(input_, weight)</code></pre></div></div><div class="section" id="section-5"><div class="docs doc-strings"><p>        前向传播： $$output = input \cdot weight^T + bias$$</p></div><div class="code"><pre><code id="code_5" name="py_code">        output = input_.mm(weight.t())
        output += bias
        return output
</code></pre></div></div><div class="section" id="section-6"><div class="docs doc-strings"><p>        <b>backward 函数功能概述</b><br>            线性函数的反向传播计算过程。</p></div><div class="code"><pre><code id="code_6" name="py_code">    @staticmethod
    def backward(ctx, grad_output):</code></pre></div></div><div class="section" id="section-8"><div class="docs doc-strings"><p>        拿回在前向传播中保存的参数。</p></div><div class="code"><pre><code id="code_8" name="py_code">        input_, weight = ctx.saved_tensors</code></pre></div></div><div class="section" id="section-9"><div class="docs doc-strings"><p>        初始化梯度为 None。这是因为并不是所有的参数都需要被求导，如果某参数无需被求导，其梯度应当返回 None。</p></div><div class="code"><pre><code id="code_9" name="py_code">        grad_input, grad_weight, grad_bias = None, None, None</code></pre></div></div><div class="section" id="section-10"><div class="docs doc-strings"><p>        对输入 input 进行反向传播： $$ \nabla input = \nabla output \cdot weight $$</p></div><div class="code"><pre><code id="code_10" name="py_code">        if ctx.needs_input_grad[0]:
            grad_input = grad_output.mm(weight)</code></pre></div></div><div class="section" id="section-11"><div class="docs doc-strings"><p>        对权重 weight 进行反向传播： $$ \nabla weight = \nabla output^T \cdot input $$</p></div><div class="code"><pre><code id="code_11" name="py_code">        if ctx.needs_input_grad[1]:
            grad_weight = grad_output.t().mm(input_)</code></pre></div></div><div class="section" id="section-12"><div class="docs doc-strings"><p>        对权重 bias 进行反向传播： $$ \nabla bias = \sum \nabla output $$</p></div><div class="code"><pre><code id="code_12" name="py_code">        if ctx.needs_input_grad[2]:
            grad_bias = grad_output.sum(0)
        return grad_input, grad_weight, grad_bias

</code></pre></div></div><div class="section" id="section-13"><div class="docs doc-strings"><p>    <b>test_linear_function 函数功能概述</b><br>        测试定义的线性函数，对前向传播结果，以及反向传播结果进行结果检查。</p></div><div class="code"><pre><code id="code_13" name="py_code">def test_linear_function():</code></pre></div></div><div class="section" id="section-15"><div class="docs doc-strings"><p>    生成测试数据。</p></div><div class="code"><pre><code id="code_15" name="py_code">    w = torch.randn(4, 3, requires_grad=True)
    x = torch.randn(1, 3, requires_grad=False)
    b = torch.randn(4, requires_grad=True)
</code></pre></div></div><div class="section" id="section-16"><div class="docs doc-strings"><p>    使用 PyTorch 内置方法完成线性计算。</p></div><div class="code"><pre><code id="code_16" name="py_code">    o = torch.sum(x @ w.t() + b)</code></pre></div></div><div class="section" id="section-17"><div class="docs doc-strings"><p>    使用 PyTorch 内置的自动求导完成反向传播。</p></div><div class="code"><pre><code id="code_17" name="py_code">    o.backward()</code></pre></div></div><div class="section" id="section-18"><div class="docs doc-strings"><p>    保留反向传播结果，用于后续结果检查。</p></div><div class="code"><pre><code id="code_18" name="py_code">    w_grad, b_grad = deepcopy(w.grad), deepcopy(b.grad)
    w.grad, x.grad, b.grad = None, None, None
</code></pre></div></div><div class="section" id="section-19"><div class="docs doc-strings"><p>    使用自定义的线性函数进行前向传播。</p></div><div class="code"><pre><code id="code_19" name="py_code">    linear_func = LinearFunction()
    o = torch.sum(linear_func.apply(x, w, b))</code></pre></div></div><div class="section" id="section-20"><div class="docs doc-strings"><p>    反向传播。</p></div><div class="code"><pre><code id="code_20" name="py_code">    o.backward()
</code></pre></div></div><div class="section" id="section-21"><div class="docs doc-strings"><p>    对求导的结果进行正确性检查。</p></div><div class="code"><pre><code id="code_21" name="py_code">    assert x.grad is None
    assert torch.sum(torch.abs(w_grad - w.grad)) < 1e-6
    assert torch.sum(torch.abs(b_grad - b.grad)) < 1e-6

</code></pre></div></div><div class="section" id="section-22"><div class="docs doc-strings"><p>    <b>test_auto_grad 函数功能概述</b><br>        测试自动求导的机制，对比用 Numpy 的手写求导与 PyTorch 的自动求导结果。</p></div><div class="code"><pre><code id="code_22" name="py_code">def test_auto_grad():</code></pre></div></div><div class="section" id="section-24"><div class="docs doc-strings"><p>    规定测试数据的格式。</p></div><div class="code"><pre><code id="code_24" name="py_code">    B, D = 3, 4</code></pre></div></div><div class="section" id="section-25"><div class="docs doc-strings"><p>    生成 Numpy 版本的测试数据。</p></div><div class="code"><pre><code id="code_25" name="py_code">    x = np.random.randn(B, D)
    y = np.random.randn(B, D)
    z = np.random.randn(B, D)</code></pre></div></div><div class="section" id="section-26"><div class="docs doc-strings"><p>    Numpy 版本的前向传播。</p></div><div class="code"><pre><code id="code_26" name="py_code">    a = x * y
    b = a + z
    c = np.sum(b)</code></pre></div></div><div class="section" id="section-27"><div class="docs doc-strings"><p>    Numpy 版本的反向传播。</p></div><div class="code"><pre><code id="code_27" name="py_code">    grad_c = 1.0
    grad_b = grad_c * np.ones((B, D))
    grad_a = grad_b.copy()
    grad_z = grad_b.copy()
    grad_x = grad_a * y
    grad_y = grad_a * x</code></pre></div></div><div class="section" id="section-28"><div class="docs doc-strings"><p>    将 Numpy 版本的测试数据转化为 PyTorch 版本。</p></div><div class="code"><pre><code id="code_28" name="py_code">    x = nn.Parameter(torch.from_numpy(x)).requires_grad_(True)
    y = nn.Parameter(torch.from_numpy(y)).requires_grad_(True)
    z = nn.Parameter(torch.from_numpy(z)).requires_grad_(True)</code></pre></div></div><div class="section" id="section-29"><div class="docs doc-strings"><p>    PyTorch 版本的前向传播。</p></div><div class="code"><pre><code id="code_29" name="py_code">    a = x * y
    b = a + z
    c = torch.sum(b)</code></pre></div></div><div class="section" id="section-30"><div class="docs doc-strings"><p>    PyTorch 版本的反向传播。</p></div><div class="code"><pre><code id="code_30" name="py_code">    c.backward()</code></pre></div></div><div class="section" id="section-31"><div class="docs doc-strings"><p>    检查求导的结果是否一致。</p></div><div class="code"><pre><code id="code_31" name="py_code">    assert torch.sum(torch.abs(torch.from_numpy(grad_x) - x.grad)) < 1e-6
    assert torch.sum(torch.abs(torch.from_numpy(grad_y) - y.grad)) < 1e-6
    assert torch.sum(torch.abs(torch.from_numpy(grad_z) - z.grad)) < 1e-6

</code></pre></div></div><div class="section" id="section-31"><div class="docs doc-strings"><p><i>如果读者关于本文档有任何问题和建议，可以在 GitHub 提 issue 或是直接发邮件给我们 (opendilab@pjlab.org.cn) 。</i></p></div></div></body><script type="text/javascript">
window.onload = function(){
    var codeElement = document.getElementsByName('py_code');
    var lineCount = 1;
    for (var i = 0; i < codeElement.length; i++) {
        var code = codeElement[i].innerText;
        if (code.length <= 1) {
            continue;
        }

        codeElement[i].innerHTML = "";

        var codeMirror = CodeMirror(
          codeElement[i],
          {
            value: code,
            mode: "python",
            theme: "solarized dark",
            lineNumbers: true,
            firstLineNumber: lineCount,
            readOnly: false,
            lineWrapping: true,
          }
        );
        var noNewLineCode = code.replace(/[\r\n]/g, "");
        lineCount += code.length - noNewLineCode.length + 1;
    }
};
</script></html>