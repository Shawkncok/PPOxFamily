<!DOCTYPE html>
<html><head><meta charset="utf-8"></meta><title>Annonated Algorithm Visualization</title><link rel="stylesheet" href="pylit.css?v=1"></link><link rel="stylesheet" href="solarized.css"></link><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" integrity="sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/" crossorigin="anonymous"></link><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js" integrity="sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);" defer="True"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.css"></link><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.js"></script><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/mode/python/python.min.js"></script></head><body><div class="section" id="section-0"><div class="docs doc-strings"><p><p><a href="index.html"><b>HOME<br></b></a></p></p><a href="https://github.com/opendilab/PPOxFamily" target="_blank"><img alt="GitHub" style="max-width:100%;" src="https://img.shields.io/github/stars/opendilab/PPOxFamily?style=social"></img></a>  <a href="https://space.bilibili.com/1112854351?spm_id_from=333.337.0.0" target="_blank"><img alt="bilibili" style="max-width:100%;" src="https://img.shields.io/badge/bilibili-video%20course-blue"></img></a>  <a href="https://twitter.com/OpenDILab" rel="nofollow" target="_blank"><img alt="twitter" style="max-width:100%;" src="https://img.shields.io/twitter/follow/opendilab?style=social"></img></a><br><a href="https://github.com/opendilab/PPOxFamily/tree/main/chapter4_reward/value_rescale.py" target="_blank">View code on GitHub</a><br><br>Typically, we need to apply normalization functions in RL training to reduce the scale of some predictions of neural networks (e.g. value function) to enhance the RL training process.<br>In this document, we will demonstrate two kinds of data normalization methods and their corresponding inverse operations.<br>- The first one is <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">value_transform</span> , which can reduce the scale of the action-value function. Its corresponding inverse operation is <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">value_inv_transform</span> . <a href="https://arxiv.org/pdf/1805.11593.pdf">Related Link</a><br>- The second one is <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">symlog</span> , which is another approach to normalize the input tensor. Its corresponding inverse operation is <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">inv_symlog</span> . <a href="https://arxiv.org/pdf/2301.04104.pdf">Related Link</a></div></div><div class="section" id="section-1"><div class="docs doc-strings"><p>    <b>Overview</b><br>        A function to reduce the scale of the action-value function. For extensive reading, please refer to: Achieving Consistent Performance on Atari <a href="https://arxiv.org/abs/1805.11593">Related Link</a><br>        Given the input tensor <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">x</span> , this function will return the normalized tensor.<br>        The argument <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">eps</span> is a hyper-parameter that controls the additive regularization term to ensure the corresponding inverse operation is Lipschitz continuous.</p></div><div class="code"><pre><code id="code_1" name="py_code">import torch


def value_transform(x: torch.Tensor, eps: float = 1e-2) -> torch.Tensor:</code></pre></div></div><div class="section" id="section-3"><div class="docs doc-strings"><p>    Core implementation.<br>    The formula of the normalization is: $$h(x) = sign(x)(\sqrt{(|x|+1)} - 1) + \epsilon * x$$</p></div><div class="code"><pre><code id="code_3" name="py_code">    return torch.sign(x) * (torch.sqrt(torch.abs(x) + 1) - 1) + eps * x

</code></pre></div></div><div class="section" id="section-4"><div class="docs doc-strings"><p>    <b>Overview</b><br>        The inverse form of value transform. Given the input tensor <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">x</span> , this function will return the unnormalized tensor.</p></div><div class="code"><pre><code id="code_4" name="py_code">def value_inv_transform(x: torch.Tensor, eps: float = 1e-2) -> torch.Tensor:</code></pre></div></div><div class="section" id="section-6"><div class="docs doc-strings"><p>    The formula of the unnormalization is: $$h^{-1}(x) = sign(x)({(\frac{\sqrt{1+4\epsilon(|x|+1+\epsilon)}-1}{2\epsilon})}^2-1)$$</p></div><div class="code"><pre><code id="code_6" name="py_code">    return torch.sign(x) * (((torch.sqrt(1 + 4 * eps * (torch.abs(x) + 1 + eps)) - 1) / (2 * eps)) ** 2 - 1)

</code></pre></div></div><div class="section" id="section-7"><div class="docs doc-strings"><p>    <b>Overview</b><br>        A function to normalize the targets. For extensive reading, please refer to: Mastering Diverse Domains through World Models <a href="https://arxiv.org/abs/2301.04104">Related Link</a><br>        Given the input tensor <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">x</span> , this function will return the normalized tensor.</p></div><div class="code"><pre><code id="code_7" name="py_code">def symlog(x: torch.Tensor) -> torch.Tensor:</code></pre></div></div><div class="section" id="section-9"><div class="docs doc-strings"><p>    The formula of the normalization is: $$symlog(x) = sign(x)(\ln{|x|+1})$$</p></div><div class="code"><pre><code id="code_9" name="py_code">    return torch.sign(x) * (torch.log(torch.abs(x) + 1))

</code></pre></div></div><div class="section" id="section-10"><div class="docs doc-strings"><p>    <b>Overview</b><br>        The inverse form of symlog. Given the input tensor <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">x</span> , this function will return the unnormalized tensor.</p></div><div class="code"><pre><code id="code_10" name="py_code">def inv_symlog(x: torch.Tensor) -> torch.Tensor:</code></pre></div></div><div class="section" id="section-12"><div class="docs doc-strings"><p>    The formula of the unnormalization is: $$symexp(x) = sign(x)(\exp{|x|}-1)$$</p></div><div class="code"><pre><code id="code_12" name="py_code">    return torch.sign(x) * (torch.exp(torch.abs(x)) - 1)

</code></pre></div></div><div class="section" id="section-13"><div class="docs doc-strings"><p>    <b>Overview</b><br>        Generate fake data and test the <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">value_transform</span> and <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">value_inv_transform</span> functions.</p></div><div class="code"><pre><code id="code_13" name="py_code">def test_value_transform():</code></pre></div></div><div class="section" id="section-15"><div class="docs doc-strings"><p>    Generate fake data.</p></div><div class="code"><pre><code id="code_15" name="py_code">    test_x = torch.randn(10)</code></pre></div></div><div class="section" id="section-16"><div class="docs doc-strings"><p>    Normalize the generated data.</p></div><div class="code"><pre><code id="code_16" name="py_code">    normalized_x = value_transform(test_x)
    assert normalized_x.shape == (10,)</code></pre></div></div><div class="section" id="section-17"><div class="docs doc-strings"><p>    Unnormalize the data.</p></div><div class="code"><pre><code id="code_17" name="py_code">    unnormalized_x = value_inv_transform(normalized_x)</code></pre></div></div><div class="section" id="section-18"><div class="docs doc-strings"><p>    Test whether the data before and after the transformation is the same.</p></div><div class="code"><pre><code id="code_18" name="py_code">    assert torch.sum(torch.abs(test_x - unnormalized_x)) < 1e-3

</code></pre></div></div><div class="section" id="section-19"><div class="docs doc-strings"><p>    <b>Overview</b><br>        Generate fake data and test the <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">symlog</span> and <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">inv_symlog</span> functions.</p></div><div class="code"><pre><code id="code_19" name="py_code">def test_symlog():</code></pre></div></div><div class="section" id="section-21"><div class="docs doc-strings"><p>    Generate fake data.</p></div><div class="code"><pre><code id="code_21" name="py_code">    test_x = torch.randn(10)</code></pre></div></div><div class="section" id="section-22"><div class="docs doc-strings"><p>    Normalize the generated data.</p></div><div class="code"><pre><code id="code_22" name="py_code">    normalized_x = symlog(test_x)
    assert normalized_x.shape == (10,)</code></pre></div></div><div class="section" id="section-23"><div class="docs doc-strings"><p>    Unnormalize the data.</p></div><div class="code"><pre><code id="code_23" name="py_code">    unnormalized_x = inv_symlog(normalized_x)</code></pre></div></div><div class="section" id="section-24"><div class="docs doc-strings"><p>    Test whether the data before and after the transformation is the same.</p></div><div class="code"><pre><code id="code_24" name="py_code">    assert torch.sum(torch.abs(test_x - unnormalized_x)) < 1e-3

</code></pre></div></div><div class="section" id="section-24"><div class="docs doc-strings"><p><i>If you have any questions or advices about this documation, you can raise issues in GitHub (https://github.com/opendilab/PPOxFamily) or email us (opendilab@pjlab.org.cn).</i></p></div></div></body><script type="text/javascript">
window.onload = function(){
    var codeElement = document.getElementsByName('py_code');
    var lineCount = 1;
    for (var i = 0; i < codeElement.length; i++) {
        var code = codeElement[i].innerText;
        if (code.length <= 1) {
            continue;
        }

        codeElement[i].innerHTML = "";

        var codeMirror = CodeMirror(
          codeElement[i],
          {
            value: code,
            mode: "python",
            theme: "solarized dark",
            lineNumbers: true,
            firstLineNumber: lineCount,
            readOnly: false,
            lineWrapping: true,
          }
        );
        var noNewLineCode = code.replace(/[\r\n]/g, "");
        lineCount += code.length - noNewLineCode.length + 1;
    }
};
</script></html>